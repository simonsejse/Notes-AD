

\section{Rules of arithmetic}
\subsection{Logarithms}

\begin{align}
  (\log_b(x))^a     & = \log_b(x^a)           \\
  \log_b(x^a)       & = a \cdot \log_b(x)     \\
  \log_b(x \cdot y) & = \log_b(x) + \log_b(y) \\
  \log_b(x / y)     & = \log_b(x) - \log_b(y) \\
  \log_b(1)         & = 0                     \\
  \log_b(b)         & = 1                     \\
  \log_b(b^x)       & = x                     \\
  b^{\log_b(x)}     & = x
\end{align}

\section{5/2: Introduction, Proofs, Loop Invariants, CLRS 2.1}
\addquestionsection{5/2: Introduction, Proofs, Loop Invariants}

\begin{tcolorbox}[title=Chapter Quick Summary Notes (CRLS 2.1)]
  This box can contain summaries, key points, and important definitions from the reading material assigned (CLRS 2.1). Highlight the main concepts, proofs, and examples discussed in the text.
\end{tcolorbox}

\subsection{Loop Invariants:}
Loop invariants are used to prove the correctness of algorithms. They are used to show that the algorithm is working as expected. The loop invariant is a property that holds before and after each iteration of the loop. When you're using a loop invariant, you need to show three things:

\begin{enumerate}
  \item \textbf{Initialization:} It is true prior to the first iteration of the loop.
  \item \textbf{Maintenance:} If it is true before an iteration of the loop, it remains true before the next iteration.
  \item \textbf{Termination:} The loop terminates, and when it terminates, the invariant - usually along with the reason that the loop terminated - gives us a useful property that helps show that the algorithm is correct.
\end{enumerate}
When the first two properties are true, the loop invariant is true prior to every iterarion of the loop. When the third property is true, the loop invariant is true after the last iteration of the loop, giving us a useful property that helps show that the algorithm is correct.
\\ \\
A loop-invariant proof is a form of mathematical induction, where to prove that a property holds, you prove a base case and an inductive step. Here, showing the invariant holds before the first iteration of the loop is like the base case, and showing that the invariant holds from one iteration to the next (from iteration to iteration) is like the inductive step. The third property is perhaps the most important one, because it is where the loop invariant is used to show that the algorithm is correct.



\subsection{Proofs:}
Test.

\subsection{Insertion-Sort and cost analysis}

\begin{algorithm}[H]
  \caption{Insertion-Sort(A)}
  \AlgoTopMargin
  \AlgoInput{$A[1..n]$, an array of numbers.} \\
  \AlgoOutput{$A[1..n]$, sorted in non-decreasing order.}
  \begin{multicols}{2}
    \begin{algorithmic}[1]
      \For{\texttt{j = 2 to A.length}}
      \State \texttt{key = A[j]}
      \State \texttt{i = j - 1}
      \While{\texttt{i > 0 and A[i] > key}}
      \State \texttt{A[i + 1] = A[i]}
      \State \texttt{i = i - 1}
      \EndWhile
      \State \texttt{A[i + 1] = key}
      \EndFor
    \end{algorithmic}

    \columnbreak
    \vspace*{-8.8em}
    \begin{align*}
       & \text{cost} &  & \text{times}                       \\[3.8em]
       & c_1         &  & n                                  \\[-0.4em]
       & c_2         &  & n - 1                              \\[-0.2em]
       & 0           &  & n - 1                              \\[-0.3em]
       & c_4         &  & n - 1                              \\[0.68em]
       & c_5         &  & \textstyle \sum_{j=2}^{n}t_j       \\[-0.3em]
       & c_6         &  & \textstyle \sum_{j=2}^{n}(t_j - 1) \\[-0.4em]
       & c_7         &  & \textstyle \sum_{j=2}^{n}(t_j - 1) \\
    \end{align*}
  \end{multicols}
\end{algorithm}
where $t_j$ is the number of times the while loop test in line 4 is executed for that value of $j$. $T(n)$ also called the recurrence relation, is given by:
\begin{align*}
  T(n) & = c_1n + c_2(n - 1) + c_4(n - 1) + c_5\sum_{j=2}^{n}t_j + c_6\sum_{j=2}^{n}(t_j - 1) + c_7\sum_{j=2}^{n}(t_j - 1) \\
       & = (c_1 + c_2 + c_4)n - (c_2 + c_4 + c_5 + c_6 + c_7) + (c_5 + c_6 + c_7)\sum_{j=2}^{n}(t_j - 1)
\end{align*}
For the worst case we have $t_j = j$ for $j = 2, 3, \ldots, n$ because each $j$ iteration it has to move the key to the first position and we know that $\textstyle \sum_{i=1}^n = \frac{n(n - 1)}{2}$ and so hence we would have $T(n)$ as follows:
\begin{align*}
  T(n) & = (c_1 + c_2 + c_4)n - (c_2 + c_4 + c_5 + c_6 + c_7) + (c_5 + c_6 + c_7)\sum_{j=2}^{n}(j - 1)                     \\
       & = (c_1 + c_2 + c_4)n - (c_2 + c_4 + c_5 + c_6 + c_7) + (c_5 + c_6 + c_7)\left(\frac{n(n - 1)}{2} - (n - 1)\right) \\
       & = an^2 + bn + c
\end{align*}
And thereby we have shown that the worst-case running time of insertion-sort is $\Theta(n^2)$.


\subsection{Exercises:}

\qs{Proof (direct proof)}{
  Prove that \( x^2 - y^2 = 1 \) has no solutions for positive integers \( x,y \) (the positive integers are the numbers \( 1,2,\ldots \)).
}

\pf{Proof for Question 1}{
  We can consider the equation in the form of difference of two squares:
  \[
    x^2 - y^2 = (x - y)(x + y) = 1
  \]
  We can now recall that for positive integers a and b, if $ab = 1$, then both $a$ and $b$ must be $1$. Since $1$ is the only positive integer multiplied by itself that gives $1$. Hence, when we consider the equation $(x - y)(x + y) = 1$, we can see that the only way to get $1$ is if $x - y = 1$ and $x + y = 1$. This gives us the system of equations:
  \[
    \begin{cases}
      x - y = 1 \\
      x + y = 1
    \end{cases}
  \]
  Adding the two equations gives us $2x + 0y = 2x = 2$, and so $x = 1$. Substituting $x = 1$ into the second equation gives us $1 + y = 1$, and so $y = 0$. But $0$ is not a positive integer, and so the equation \( x^2 - y^2 = 1 \) has no solutions for positive integers \( x,y \).
}

\qs{Proof (direct proof)}{
  Prove that \( x^2 - y^2 = 10 \) has no solutions for positive integers \( x,y \)
}

\pf{Proof for Question 2}{
  Again, we can consider the equation in the form of difference of two squares:
  \[
    x^2 - y^2 = (x - y)(x + y) = 10
  \]
  This time we will in fact assume that $\exists x, y \in \mathbb{Z}^+$ such that $x^2 - y^2 = 10$. If we consider the equation $(x - y)(x + y) = 10$, there are four ways to factorize $10$ into two numbers, and they are:
  \[
    \begin{cases}
      x - y = 1, x + y = 10 \\
      x - y = 2, x + y = 5  \\
      x - y = 5, x + y = 2  \\
      x - y = 10, x + y = 1
    \end{cases}
  \]
  But because of symmetry, we can see that the first and last equations are the same, and the second and third equations are the same. If we solve the first equations by adding the two equations:
  \[
    x - y + x + y = 1 + 10 \Rightarrow 2x = 11 \Rightarrow x = \frac{11}{2}
  \]

  But $x$ is a positive integer, and so the first equation has no solutions. We can then also solve for the second equations by adding the two equations:
  \[
    x - y + x + y = 2 + 5 \Rightarrow 2x = 7 \Rightarrow x = \frac{7}{2}
  \]
  But $x$ is a rational number, and so the second equation has no solutions.
}

\qs{Proof (proof by contradiction)}{
  Prove that if \( a \) is a rational number and \( b \) is an irrational number, then \( a + b \) is an irrational number. (A rational number is one that can be written in the form \( \frac{x}{y} \), for some \( x, y \in \mathbb{Z} \) and \( y \neq 0 \)).
}

\pf{Proof for Question 3}{
  We can prove this by contradiction. Assume that \( a + b \) is a rational number. Then \( a + b = \frac{x}{y} \) for some \( x, y \in \mathbb{Z} \) and \( y \neq 0 \). We know that $a$ is a rational number, hence $a=\frac{p}{q}$ for some $p, q \in \mathbb{Z}$ and $q \neq 0$, and so $b$ could be written as:
  \begin{align*}
    b & = \frac{x}{y} - \frac{p}{q} = \frac{xq - py}{qy}
  \end{align*}
  But this is a rational number, which is a contradiction because b was assumed to be irrational. Therefore, the initial assumption that $a+b$ is rational must be false, so $a+b$ is irrational.
}

\qs{Proof (\maltese)}{
  Give a formula for the sum of the first \( n \) odd numbers.
  \[
    f(n) = \sum_{i=1}^{n} (2i - 1)
  \]
}


\qs{Proof (proof by induction)}{
  Show that \( n! > 2^n \) for \( n \geq 4 \).
}

\pf{Proof for Question 5}{
  Let's prove by induction. First we will test our base case, which will be:
  \begin{align*}
    4! & > 2^4 \\
    24 & > 16
  \end{align*}
  This holds, and now we can assume that \( n! > 2^n \) for some \( n \geq 4 \). We can then show that \( (n + 1)! > 2^{n + 1} \) as follows:
  \begin{align*}
    (n+1)!        & > 2^{n + 1}     \\
    n!\cdot (n+1) & > 2^n \cdot 2^1
  \end{align*}
  We know that \( n! > 2^n \) by our assumption and $(n+1) \geq 2^1$ hence the inequality holds and thereby the proof is complete and \( n! > 2^n \) for \( n \geq 4 \).
}

\qs{Proof (proof by induction)}{
  The Fibonacci numbers are defined as \( f_0 = 0, f_1 = 1 \), and \( f_{n} = f_{n-1} + f_{n-2} \), for \( n \geq 2 \). The first Fibonacci numbers are thus \( 0, 1, 1, 2, 3, 5, 8, 13, 21, \ldots \)
  Prove that
  \[
    \sum_{i=1}^{n} f_i^2 = f_n f_{n+1}
  \]
}


\pf{Proof for Question 6}{
We will prove this by induction. First we will test our base case, which will be:
\begin{align*}
  \sum_{i=1}^{2} f_i^2 & = f_2 f_{2+1} \\
  f_{1}^2 + f_{2}^2    & = f_2 f_{3}   \\
  1^2 + 1^2            & = 1 \cdot 2   \\
  2                    & = 2
\end{align*}
This holds, and now we can assume that \( \textstyle \sum_{i=1}^{n} f_i^2 = f_n f_{n+1} \) holds for some \( n \geq 2 \). We can then show that \( \textstyle \sum_{i=1}^{n+1} f_i^2 \) holds as well:
\begin{align*}
  \sum_{i=1}^{n+1} f_i^2 & = \sum_{i=1}^{n} f_i^2 + f_{n+1}^2 \\
  \sum_{i=1}^{n+1} f_i^2 & = f_n f_{n+1} + f_{n+1}^2          \\
  \sum_{i=1}^{n+1} f_i^2 & = f_{n+1}(f_n + f_{n+1})           \\
  \sum_{i=1}^{n+1} f_i^2 & = f_{n+1}(f_n + f_{n+1})           \\
  \sum_{i=1}^{n+1} f_i^2 & = f_{n+1}f_{n+2}
\end{align*}
After proving the base case and assuming the induction hypothesis \( \textstyle \sum_{i=1}^{n} f_i^2 = f_n f_{n+1} \) holds for some \( n \geq 2 \), we have shown that \( \textstyle \sum_{i=1}^{n+1} f_i^2 = f_{n+1}f_{n+2} \) holds as well. Therefore, the proof is complete.
}

\qs{Proof (very good induction proof)}{
  Show that
  \[
    \sum_{i=1}^{n} f_{i} = f_{n+2} - 1
  \]
}

\pf{Proof for Question 7}{
  We will prove this by induction. First we will test our base case, which will be:
  \begin{align*}
    \sum_{i=1}^{2} f_{i} & = f_{2+2} - 1 \\
    f_1 + f_2            & = f_4 - 1     \\
    1 + 1                & = 3 - 1       \\
    2                    & = 2
  \end{align*}
  This holds, and now we can assume that \( \textstyle \sum_{i=1}^{n} f_{i} = f_{n+2} - 1 \) holds for some \( n \geq 2 \). We can then show that \( \textstyle \sum_{i=1}^{n+1} f_{i} \) holds as well:
  \begin{align*}
    \sum_{i=1}^{n+1} f_{i} & = \sum_{i=1}^{n} f_{i} + f_{n+1} \\
    \sum_{i=1}^{n+1} f_{i} & = f_{n+2} - 1 + f_{n+1}          \\
    \sum_{i=1}^{n+1} f_{i} & = f_{n+2} + f_{n+1} - 1          \\
    \sum_{i=1}^{n+1} f_{i} & = f_{n+3} - 1
  \end{align*}
  After proving the base case and assuming the induction hypothesis \( \textstyle \sum_{i=1}^{n} f_{i} = f_{n+2} - 1 \) holds for some \( n \geq 2 \), we have shown that \( \textstyle \sum_{i=1}^{n+1} f_{i} = f_{n+3} - 1 \) holds as well. Therefore, the proof is complete.
}

\qs{Proof (very nice induction proof too)}{
  Show that
  \[
    \sum_{i=1}^{n} \frac{1}{i(i+1)} = \frac{n}{n+1}
  \]
}

\pf{Proof for Question 8}{
  We will prove this by induction. First we will test our base case, which will be:
  \begin{align*}
    \sum_{i=1}^{1} \frac{1}{i(i+1)} & = \frac{1}{1+1} \\
    \frac{1}{1(1+1)}                & = \frac{1}{2}   \\
    \frac{1}{2}                     & = \frac{1}{2}
  \end{align*}
  This holds, and now we can assume that \( \textstyle \sum_{i=1}^{n} \frac{1}{i(i+1)} = \frac{n}{n+1} \) holds for some \( n \geq 1 \). We can then show that \( \textstyle \sum_{i=1}^{n+1} \frac{1}{i(i+1)} \) holds as well:
  \begin{align*}
    \sum_{i=1}^{n+1} \frac{1}{i(i+1)} & = \sum_{i=1}^{n} \frac{1}{i(i+1)} + \frac{1}{(n+1)(n+1+1)}             \\
    \sum_{i=1}^{n+1} \frac{1}{i(i+1)} & = \frac{n}{n+1} + \frac{1}{(n+1)(n+2)}                                 \\
    \sum_{i=1}^{n+1} \frac{1}{i(i+1)} & = \frac{n(n+1)(n+2)}{(n+1)(n+1)(n+2)} + \frac{1(n+1)}{(n+1)(n+1)(n+2)} \\
    \sum_{i=1}^{n+1} \frac{1}{i(i+1)} & = \frac{n(n+1)(n+2) + (n+1)}{(n+1)(n+1)(n+2)}                          \\
    \sum_{i=1}^{n+1} \frac{1}{i(i+1)} & = \frac{n(n+2) + 1}{(n+1)(n+2)}                                        \\
    \sum_{i=1}^{n+1} \frac{1}{i(i+1)} & = \frac{n^2 + 2n + 1}{(n+1)(n+2)}                                      \\
    \sum_{i=1}^{n+1} \frac{1}{i(i+1)} & = \frac{(n+1)^2}{(n+1)(n+2)}                                           \\
    \sum_{i=1}^{n+1} \frac{1}{i(i+1)} & = \frac{n+1}{n+2}
  \end{align*}
  After proving the base case and assuming the induction hypothesis \( \textstyle \sum_{i=1}^{n} \frac{1}{i(i+1)} = \frac{n}{n+1} \) holds for some \( n \geq 1 \), we have shown that \( \textstyle \sum_{i=1}^{n+1} \frac{1}{i(i+1)} = \frac{n+1}{n+2} \) holds as well. Therefore, the proof is complete.
}

\qs{Invariants (Opgave 19, A\&D (maj 2021), AU)}{
  Given a non-negative integer \( x \) and a positive integer \( y \), the following algorithm calculates \( \floor{x/y} \):

  \begin{algorithm}[H]
    \caption{Division(x, y)}
    \AlgoTopMargin
    \AlgoInput{Integer \( x \geq 0\) and \( y \geq 1 \).} \\
    \AlgoOutput{$r=\floor{x/y}$}
    \begin{algorithmic}[1]
      \State \( r \gets 0 \)
      \While{\( x \geq y \)}
      \State \( x \gets x - y \)
      \State \( r \gets r + 1 \)
      \EndWhile
    \end{algorithmic}
    \label{alg:division(x, y)}
  \end{algorithm}
}

The following 5 loop environments are given explain if they are loop invariants or not:
\begin{enumerate}
  \item $r = \floor{x/y}$. \\
        \textbf{Answer:} No. $r$ is initialized to $0$ and the loop invariant states at initialization that $r = \floor{x/y}$, which is simply just not true. And for Maintenance it wouldn't hold either because $r$ is incremented by $1$ each time the loop runs. It would hold only at termination, but that is not enough to be a loop invariant only to show that the algorithm is correct.
  \item $r = \floor{x_0/y_0}$. \\
        \textbf{Answer:} No, for the same reason as the first loop invariant. And here $x_0$ and $y_0$ are static values, which means that in the maintenance step, the values wouldn't change and could not be a loop invariant.
  \item $r=\floor{(x_0 - x)/y}$. \\
        \textbf{Answer:} Yes. This is a loop invariant. It holds at initialization, maintenance, and termination. At initialization, $x = x_0$ and so $r = \floor{(x_0 - x_0)/y} = \floor{0/y} = 0$. At maintenance,
  \item $x + ry = x_0$. \\
        \textbf{Answer:}
  \item $r(x - x_0) = y$. \\
        \textbf{Answer:}
\end{enumerate}

\section{7/2: Time Complexity and Asymptotic Notation, CLRS 2.2, 3}
\addtheoremsection{7/2: Time Complexity and Asymptotic Notation}

\subsection{Time Complexity and Asymptotic Notation}
In practice the notation is often used to denote a (unnamed) function, e.g.:
\begin{itemize}
  \item \( g(n) = O(n^2) \) means \( g(n) = f(n) \) for some \( f(n) \in O(n^2) \)
  \item \( g(n) = n^2 + \Omega(n) \) means \( g(n) = n^2 + f(n) \) for some \( f(n) \in \Omega(n) \)
\end{itemize}

\subsubsection{Comparison of Growth Rates for Common Functions}

\begin{equation}
  1 < \log n < \sqrt{n} < n < n \log n < n^2 < n^3 < \cdots < 2^n < 3^n < \cdots < n^n
  \label{eq:comparison-of-growth-rates}
\end{equation}

\subsubsection{Formal Definitions of Asymptotic Notation}

\paragraph{Formal definitions:} For a non negative function \( g(n) : \mathbb{N} \rightarrow \mathbb{R} \) er
\begin{align*}
  O(g(n))      & = \{ f(n) \mid \exists c > 0, n_0 > 0 : \forall n \geq n_0 \rightarrow 0 \leq f(n) \leq cg(n) \}                \\
  \Omega(g(n)) & = \{ f(n) \mid \exists c > 0, n_0 > 0 : \forall n \geq n_0 \rightarrow c g(n) \leq f(n) \}                      \\
  \Theta(g(n)) & = \{ f(n) \mid \exists c_1, c_2 > 0, n_0 > 0 : \forall n \geq n_0 \rightarrow c_1g(n) \leq f(n) \leq c_2g(n) \} \\
  o(g(n))      & = \{ f(n) \mid \forall c > 0 : \exists n_0 > 0 : \forall n \geq n_0 \Rightarrow 0 \leq f(n) < c g(n) \}         \\
  \omega(g(n)) & = \{ f(n) \mid \forall c > 0 : \exists n_0 > 0 : \forall n \geq n_0 \Rightarrow c g(n) < f(n) \}
\end{align*}


\paragraph{Little-o and little-omega:} are used to denote strict inequalities, e.g.:
\begin{align*}
  f(n) & = o(g(n)) \quad \text{(}"\textgreater{}\text{")} \\
  f(n) & = \Omega(g(n)) \quad \text{(}"\geq\text{")}      \\
  f(n) & = \Theta(g(n)) \quad \text{(}"=\text{")}         \\
  f(n) & = O(g(n)) \quad \text{(}"\leq\text{")}           \\
  f(n) & = o(g(n)) \quad \text{(}"\textless{}\text{")}
\end{align*}

\subsection{Master Theorem}

\thm{Master Theorem}{
  Let \(a \geq 1\) and \(b > 1\) be constants, let \(f(n)\) be a function, and let \(T(n)\) be defined on the non-negative integers by the recurrence
  \[T(n) = aT\left(\frac{n}{b}\right) + f(n)\]
  where we interpret \(n/b\) to mean either \(\lfloor n/b \rfloor\) or \(\lceil n/b \rceil\). Then \(T(n)\) can be asymptotically estimated as follows:
  \begin{enumerate}
    \item If \(f(n) = O(n^{\log_b a - \epsilon})\) for some \(\epsilon > 0\), then \(T(n) = \Theta(n^{\log_b a})\).
    \item If \(f(n) = \Theta(n^{\log_b a})\), then \(T(n) = \Theta(n^{\log_b a} \log n)\).
    \item If \(f(n) = \Omega(n^{\log_b a + \epsilon})\) for some \(\epsilon > 0\), and if \(a f(n/b) \leq k f(n)\) for some constant \(k < 1\) and sufficiently large \(n\), then \(T(n) = \Theta(f(n))\).
  \end{enumerate}
}


\subsection{Exercises}
\qs{Time Complexity and Asymptotic Notation}{
  Determine the whether the functions below are \(O\), \(\Omega\), or \(\Theta\) of the other functions.
}

\begin{center}
  \begin{tabular}{m{6cm} c c}
    \textbf{Statement}                          & \textbf{Yes} & \textbf{No} \\
    \hline \vspace*{0.2em}
    $(\log n)^2$ is $O(n^2)$                    & \cancel{Yes} & No          \\
    $n \cdot \log n$ is $O(n^2)$                & \cancel{Yes} & No          \\
    $\sqrt{n}$ is $O((\log n)^3)$               & Yes          & No          \\
    $1 + \log(n^2)$ is $O((\log n)^2)$          & Yes          & No          \\
    $\log n + \log(n!)$ is $O(n^2)$             & Yes          & No          \\
    $n^3$ is $O(n)$                             & Yes          & No          \\
    $\sqrt{n} \cdot \log n$ is $O(n)$           & Yes          & No          \\
    $n^3$ is $O(\log(n!))$                      & Yes          & No          \\
    $n^2$ is $O(n^{2/3})$                       & Yes          & No          \\
    $7\log n + \log(n!)$ is $O(n \cdot \log n)$ & Yes          & No          \\
    $2\log n$ is $\Omega(n^{0.01})$             & Yes          & No          \\
    $(\log n)^3 + 3^n$ is $\Omega(2^n)$         & Yes          & No          \\
  \end{tabular}
\end{center}

\section{12/2: Divide and Conquer, Recursion Equations, and Lower Bound for Sorting, CLRS 2.3, 4 up to 4.5, pages 157-160, 8.1}
\section{14/2: Amortized Analysis, CLRS 16}
\section{19/2: LSM Trees, Fibonacci Heaps, CLRS digital chapter 19
  Download digital chapter 19 (19.4 is cursory)}
\section{21/2: Dynamic Programming, CLRS 14 except for 14.2 and 14.5}
\section{26/2: Greedy Algorithms, CLRS 15}
\section{28/2: No teaching due to open house}
\section{4/3: Binary Search Trees, CLRS 12 and 13}
\section{6/3: Disjoint Sets, Plane Sweep, CLRS 19.1-3, CLRS digital chapter 33.1-2
  Download CLRS digital chapter 33.1-2}
\section{11/3: Minimum Spanning Tree, CLRS 21}
\section{13/3: Shortest Paths, CLRS 22}
\section{18/3: Computational Geometry, CLRS digital chapter 33
  Download CLRS digital chapter 33}
\section{4/3: Question Time (same time and place as lectures)}
\section{8/4 OR 10/4?: Written Exam}
